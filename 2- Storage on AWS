
Chapter 2 - Storage on AWS

Questions
1. 

What is the main purpose of Amazon Glacier? (Choose all that apply.)

Storing hot, frequently used data

Storing archival data

Storing historical or infrequently accessed data

Storing the static content of a web site

Creating a cross-region replication bucket for Amazon S3


2. 

What is the best way to protect a file in Amazon S3 against accidental delete?

Upload the files in multiple buckets so that you can restore from another when a file is deleted

Back up the files regularly to a different bucket or in a different region

Enable versioning on the S3 bucket

Use MFA for deletion

Use cross-region replication


3. 

Amazon S3 provides 99.999999999 percent durability. Which of the following are true statements? (Choose all that apply.)

The data is mirrored across multiple AZs within a region.

The data is mirrored across multiple regions to provide the durability SLA.

The data in Amazon S3 Standard is designed to handle the concurrent loss of two facilities.

The data is regularly backed up to AWS Snowball to provide the durability SLA.

The data is automatically mirrored to Amazon Glacier to achieve high availability.



4. 

To set up a cross-region replication, what statements are true? (Choose all that apply.)

The source and target bucket should be in a same region.

The source and target bucket should be in different region.

You must choose different storage classes across different regions.

You need to enable versioning and must have an IAM policy in place to replicate.

You must have at least ten files in a bucket.



5. 

You want to move all the files older than a month to S3 IA. What is the best way of doing this?

Copy all the files using the S3 copy command

Set up a lifecycle rule to move all the files to S3 IA after a month

Download the files after a month and re-upload them to another S3 bucket with IA

Copy all the files to Amazon Glacier and from Amazon Glacier copy them to S3 IA

See the answer

6. 

What are the various way you can control access to the data stored in S3? (Choose all that apply.)

By using IAM policy

By creating ACLs

By encrypting the files in a bucket

By making all the files public

By creating a separate folder for the secure files



7. 

How much data can you store on S3?

1 petabyte per account

1 exabyte per account

1 petabyte per region

1 exabyte per region

Unlimited



8. 

What are the different storage classes that Amazon S3 offers? (Choose all that apply.)

S3 Standard

S3 Global

S3 CloudFront

S3 US East

S3 IA


9. 

What is the best way to delete multiple objects from S3?

Delete the files manually using a console

Use multi-object delete

Create a policy to delete multiple files

Delete all the S3 buckets to delete the files



10. 

What is the best way to get better performance for storing several files in S3?

Create a separate folder for each file

Create separate buckets in different region

Use a partitioning strategy for storing the files

Use the formula of keeping a maximum of 100 files in the same bucket


11. 

The data across the EBS volume is mirrored across which of the following?

Multiple AZs

Multiple regions

The same AZ

EFS volumes mounted to EC2 instances


12. 

I shut down my EC2 instance, and when I started it, I lost all my data. What could be the reason for this?

The data was stored in the local instance store.

The data was stored in EBS but was not backed up to S3.

I used an HDD-backed EBS volume instead of an SSD-backed EBS volume.

I forgot to take a snapshot of the instance store.


13. 

I am running an Oracle database that is very I/O intense. My database administrator needs a minimum of 3,600 IOPS. If my system is not able to meet that number, my application won't perform optimally. How can I make sure my application always performs optimally?

Use Elastic File System since it automatically handles the performance

Use Provisioned IOPS SSD to meet the IOPS number

Use your database files in an SSD-based EBS volume and your other files in an HDD-based EBS volume

Use a general-purpose SSD under a terabyte that has a burst capability


14. 

Your application needs a shared file system that can be accessed from multiple EC2 instances across different AZs. How would you provision it?

Mount the EBS volume across multiple EC2 instances

Use an EFS instance and mount the EFS across multiple EC2 instances across multiple AZs

Access S3 from multiple EC2 instances

Use EBS with Provisioned IOPS


15. 

You want to run a mapreduce job (a part of the big data workload) for a noncritical task. Your main goal is to process it in the most cost-effective way. The task is throughput sensitive but not at all mission critical and can take a longer time. Which type of storage would you choose?

Throughput Optimized HDD (st1)

Cold HDD (sc1)

General-Purpose SSD (gp2)

Provisioned IOPS (io1)

------------------------------------------------------
Answers

1. 

B, C. Hot and frequently used data needs to be stored in Amazon S3; you can also use Amazon CloudFront to cache the frequently used data. Amazon Glacier is used to store the archive copies of the data or historical data or infrequent data. You can make lifecycle rules to move all the infrequently accessed data to Amazon Glacier. The static content of the web site can be stored in Amazon CloudFront in conjunction with Amazon S3. You can't use Amazon Glacier for a cross-region replication bucket of Amazon S3; however, you can use S3 IA or S3 RRS in addition to S3 Standard as a replication bucket for CRR.

2. 

C. You can definitely upload the file in multiple buckets, but the cost will increase the number of times you are going to store the files. Also, now you need to manage three or four times more files. What about mapping files to applications? This does not make sense. Backing up files regularly to a different bucket can help you to restore the file to some extent. What if you have uploaded a new file just after taking the backup? The correct answer is versioning since enabling versioning maintains all the versions of the file and you can restore from any version even if you have deleted the file. You can definitely use MFA for delete, but what if even with MFA you delete a wrong file? With CRR, if a DELETE request is made without specifying an object version ID, Amazon S3 adds a delete marker, which cross-region replication replicates to the destination bucket. If a DELETE request specifies a particular object version ID to delete, Amazon S3 deletes that object version in the source bucket, but it does not replicate the deletion in the destination bucket.

3. 

A, C. By default the data never leaves a region. If you have created an S3 bucket in a global region, it will always stay there unless you manually move the data to a different region. Amazon does not back up data residing in S3 to anywhere else since the data is automatically mirrored across multiple facilities. However, customers can replicate the data to a different region for additional safety. AWS Snowball is used to migrate on-premises data to S3. Amazon Glacier is the archival storage of S3, and an automatic mirror of regular Amazon S3 data does not make sense. However, you can write lifecycle rules to move historical data from Amazon S3 to Amazon Glacier.

4. 

B, D. Cross-region replication can't be used to replicate the objects in the same region. However, you can use the S3 copy command or copy the files from the console to move the objects from one bucket to another in the same region. You can choose a different class of storage for CRR; however, this option is not mandatory, and you can use the same class of storage as the source bucket as well. There is no minimum number of file restriction to enable cross-region replication. You can even use CRR when there is only one file in an Amazon S3 bucket.

5. 

B. Copying all the files using the S3 copy command is going to be a painful activity if you have millions of objects. Doing this when you can do the same thing by automatically downloading and re-uploading the files does not make any sense and wastes a lot of bandwidth and manpower. Amazon Glacier is used mainly for archival storage. You should not copy anything into Amazon Glacier unless you want to archive the files.

6. 

A, B. By encrypting the files in the bucket, you can make them secure, but it does not help in controlling the access. By making the files public, you are providing universal access to everyone. Creating a separate folder for secure files won't help because, again, you need to control the access of the separate folder.

7. 

E. Since the capacity of S3 is unlimited, you can store as much data you want there.

8. 

A, E. S3 Global is a region and not a storage class. Amazon CloudFront is a CDN and not a storage class. US East is a region and not a storage class.

9. 

B. Manually deleting the files from the console is going to take a lot of time. You can't create a policy to delete multiple files. Deleting buckets in order to delete files is not a recommended option. What if you need some files from the bucket?

10. 

C. Creating a separate folder does not improve performance. What if you need to store millions of files in these separate folders? Similarly, creating separate folders in a different region does not improve the performance. There is no such rule of storing 100 files per bucket.

11. 

C. Data stored in Amazon EBS volumes is redundantly stored in multiple physical locations in the same AZ. Amazon EBS replication is stored within the same availability zone, not across multiple zones.

12. 

A. The only possible reason is that the data was stored in a local instance store that is not persisted once the server is shut down. If the data stays in EBS, then it does not matter if you have taken the backup or not; the data will always persist. Similarly, it does not matter if it is an HDD- or SSD-backed EBS volume. You can't take a snapshot of the instance store.

13. 

B. If your workload needs a certain number of workload, then the best way would be is to use a Provisioned IOPS. That way, you can ensure the application or the workload always meets the performance metric you are looking for.

14. 

B. Use an EFS. The same EBS volume can't be mounted across multiple EC2 instances.

15. 

B. Since the workload is not critical and you want to process it in the most cost-effective way, you should choose Cold HDD. Though the workload is throughput sensitive, it is not critical and is low priority; therefore, you should not choose st1. gp2 and io1 are more expensive than other options like st1.

